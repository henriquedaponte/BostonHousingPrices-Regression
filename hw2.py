import numpy as np
import pandas as pd
import cvxpy as cp

# ====================== Utility Functions ======================

def meanSquaredError(predicted, actual):
    '''
    Calculate the mean squared error (MSE) between predicted and actual values.
    
    Parameters:
    - predicted (numpy array): The array of predicted values generated by the model.
    - actual (numpy array): The array of actual or true values.
    
    Returns:
    - int: The mean squared error, rounded to the nearest whole number, between the predicted and actual values.
    
    Notes:
    The mean squared error is computed as the average of the squared differences between predicted and actual values.
    '''
    
   # Ensure that predicted array is not empty
    if len(predicted) == 0:
        raise ValueError("Predicted array is empty.")
        
    # Ensure that there are no NaN values in predicted and actual arrays
    if np.isnan(predicted).any() or np.isnan(actual).any():
        raise ValueError("NaN values detected in input arrays.")
    
    r = actual - predicted # Residuals
    mse = np.sum(r**2) / len(predicted)
    
    return round(mse)

def preprocessData(filename, trainDataPct):
    '''
    Preprocesses the data by loading it and dividing it into training and test sets.
    
    Parameters:
    - filename (str): Path to the input data file.
    - trainDataPct (float): Percentage of data to be used for training (e.g., 0.7 for 70%).
    
    Returns:
    - tuple: 
        - X_train (numpy array): Training data attributes.
        - Y_train (numpy array): Training data target values.
        - X_test (numpy array): Test data attributes.
        - Y_test (numpy array): Test data target values.
    '''
     
    data = pd.read_csv(filename, delimiter='\t')

    # Using trainDataPct% of the data for training
    trainDataSize = int(trainDataPct * data.shape[0])
    trainData = data.iloc[:trainDataSize, :]
    X_train = trainData.iloc[:, :-1].values
    Y_train = (trainData.iloc[:, -1].values).reshape(-1, 1)

    # Using testDataPct% of the data for testing 
    testData = data.iloc[trainDataSize:, :]
    X_test = testData.iloc[:, :-1].values
    Y_test = (testData.iloc[:, -1].values).reshape(-1, 1)

    return X_train, Y_train, X_test, Y_test
    
def trainLinearModel(X_train, Y_train):
    ''''
    Trains a linear regression model using CVX optimization.
    
    Parameters:
    - X_train (numpy array): Training data attributes.
    - Y_train (numpy array): Training data target values.
    
    Returns:
    - tuple:
        - Ytrain_pred (numpy array): Predicted values for the training data.
        - beta (numpy array): Learned coefficients for the attributes.
        - alpha (float): Learned intercept term.
    '''

    # Initilizing decision variables
    alpha = cp.Variable()
    beta = cp.Variable((X_train.shape[1], 1))

    # Defining function for our predictions
    Ytrain_pred = alpha + X_train @ beta

    # Defining objective function
    objective = cp.Minimize(cp.sum_squares(Y_train - Ytrain_pred))

    # Formulating problem
    problem = cp.Problem(objective)

    # solving the problem
    problem.solve()

    return Ytrain_pred.value, beta.value, alpha.value

def trainL1Model(X_train, Y_train):
    '''
    Trains a linear regression model using L1 (Lasso) regularization with CVX optimization.
    
    Parameters:
    - X_train (numpy array): Training data attributes/features.
    - Y_train (numpy array): Training data target/output values.
    
    Returns:
    - tuple:
        - Ytrain_pred (numpy array): Predicted values for the training data using the trained L1 model.
        - beta (numpy array): Learned coefficients for the attributes in the L1 model.
        - 0 (int): Placeholder for intercept term (always returns 0 since no intercept is used in this model).
    
    Notes:
    - The L1 regularization tends to induce sparsity in the model, which means it might produce a model where many feature weights are exactly zero.
    - The function uses CVX optimization to solve the regression problem with L1 penalty.
    - This version of L1 regressor does not incorporate an intercept term, hence the return value of 0 for the intercept.
    '''

    # Initilizing decision variables
    beta = cp.Variable((X_train.shape[1], 1))

    # Defining function for our predictions
    Ytrain_pred = X_train @ beta

    # Defining objective function
    objective = cp.Minimize(cp.sum(cp.abs(Y_train - Ytrain_pred)))

    # Formulating problem
    problem = cp.Problem(objective)

    # solving the problem
    problem.solve()

    return Ytrain_pred.value, beta.value, 0

def trainPolynomialModel(X_train, Y_train):
    '''
    Trains a polynomial regression model using CVX optimization.
    
    Parameters:
    - X_train (numpy array): Training data attributes.
    - Y_train (numpy array): Training data target values.
    
    Returns:
    - tuple:
        - Ytrain_pred (numpy array): Predicted values for the training data.
        - betas (numpy array): Learned coefficients for the attributes.
        - alpha (float): Learned intercept term.
    '''

    num_features = X_train.shape[1]

    # Generating polynomial terms for the input features
    X_poly = np.hstack([X_train, X_train**2, X_train**3])
    
    # Initializing decision variables
    alpha = cp.Variable()
    betas = cp.Variable((3 * num_features, 1))
    
    # Defining function for our predictions
    Ytrain_pred = alpha + X_poly @ betas

    # Defining objective function
    objective = cp.Minimize(cp.sum_squares(Y_train - Ytrain_pred))

    # Formulating problem
    problem = cp.Problem(objective)

    # Solving the problem
    problem.solve(solver=cp.SCS)

    return Ytrain_pred.value, betas.value, alpha.value

def deployModel(filename, split, trainMethod, modelName, poly=False):
    '''
    Trains a regression model on a given dataset and computes the mean squared error for both training and test data.
    
    Parameters:
    - filename (str): Path to the input data file.
    - split (float): Percentage (expressed as a decimal, e.g., 0.3 for 30%) of data to be used for training.
    - trainMethod (function): A function that trains the model. This function should return predicted values for the training set, coefficients for the attributes, and an intercept term.
    - modelName (str): A descriptive name for the model which will be used in print statements.
    - poly (bool, optional): If True, the function assumes the model is polynomial and will generate polynomial terms for the test data. Default is False.
    
    Returns:
    None
    
    Outputs:
    The function prints the mean squared error for the training data and test data. 
    
    Notes:
    - This function assumes the input data file is tab-delimited and the target values are in the last column.
    - The preprocessData function is used to split the data and should be defined elsewhere in the code.
    - If 'poly' is set to True, the function will generate polynomial terms up to the third degree for the test data.
    
    Example:
    deployModel('data.txt', 0.3, trainLinearModel, "Linear Regression")
    deployModel('data.txt', 0.3, trainPolynomialModel, "Polynomial Regression", poly=True)
    '''

    X_train, Y_train, X_test, Y_test = preprocessData(filename, split) # Splitting with split% of data for training

    Ytrain_pred, beta, alpha = trainMethod(X_train, Y_train) # Training with split% training data on 'model' model

    if poly:
        X_test = np.hstack([X_test, X_test**2, X_test**3])
        Y_pred =  alpha + X_test @ beta # Testing 30% data of the data for training on polynomial model
        
    else:
        Y_pred =  alpha + X_test @ beta # Testing split% of the data for training on 'model' model

    print(f'Mean squared error for training data on {modelName} model ({split * 100}% of data for training): ', meanSquaredError(Ytrain_pred, Y_train))
    print(f'Mean squared error for testing data on {modelName} model ({split* 100}% of data for training): ', meanSquaredError(Y_pred, Y_test))
    print('\n')

def f_func_problem_2(x, y):
    ''''
    Function provided for Problem 2
    '''
    return 0.5*x**2 + x*y - 1.5*y**2 + 2*x + 5*y + (1/3)*y**3

def f_func_problem_3(x1, x2):
    ''''
    Function provided for Problem 3
    '''
    return x1**2 * x2 - 2*x1 * x2**2 + 4*x1 * x2 - 8

# ====================== Problem 1 ======================

deployModel('housing.txt', 0.3, trainLinearModel, 'Linear', poly=False)
deployModel('housing.txt', 0.6, trainLinearModel, 'Linear', poly=False)
deployModel('housing.txt', 0.3, trainPolynomialModel, 'Polynomial', poly=True)
deployModel('housing.txt', 0.6, trainPolynomialModel, 'Polynomial', poly=True)
deployModel('housing.txt', 0.3, trainL1Model, 'L1 Regressor', poly=False)
deployModel('housing.txt', 0.6, trainL1Model, 'L1 Regressor', poly=False)


# ====================== Problem 2 ======================

import matplotlib.pyplot as plt

critical_points_problem_2 = [(-5.0, 3.0), (-3.0, 1.0)]
classification_problem_2 = {(-5.0, 3.0): 'local minimizer', (-3.0, 1.0): 'saddle point'}


# Generate x and y values
x_values = np.linspace(-10, 10, 400)
y_values = np.linspace(-10, 10, 400)
x_mesh, y_mesh = np.meshgrid(x_values, y_values)

# Evaluate the function over the grid
f_values_2 = f_func_problem_2(x_mesh, y_mesh)

# Plot the surface
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(x_mesh, y_mesh, f_values_2, cmap='viridis', alpha=0.7)
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('f(x, y)')
ax.set_title('Surface Plot of f(x, y)')
ax.view_init(elev=25, azim=-60)

# Mark the critical points on the plot
for point, label in classification_problem_2.items():
    ax.scatter(*point, f_func_problem_2(*point), color='r', s=50, label=f"{label} {point}")

# Show the plot
plt.legend()
plt.show()


# ====================== Problem 3 ======================

critical_points_problem_3 = [(-4, 0), (-4/3, 2/3), (0, 0), (0, 2)]
classification_problem_3 = {(-4, 0): 'saddle point', (-4/3, 2/3): 'local minimizer', (0, 0): 'saddle point', (0, 2): 'saddle point'}

# Generate x1 and x2 values
x1_values = np.linspace(-5, 5, 400)
x2_values = np.linspace(-5, 5, 400)
x1_mesh, x2_mesh = np.meshgrid(x1_values, x2_values)

# Evaluate the function over the grid
f_values = f_func_problem_3(x1_mesh, x2_mesh)

# Plot the surface
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(x1_mesh, x2_mesh, f_values, cmap='viridis', alpha=0.7)
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('f(x1, x2)')
ax.set_title('Surface Plot of f(x1, x2)')
ax.view_init(elev=25, azim=-60)

# Mark the critical points on the plot
for point, label in classification_problem_3.items():
    ax.scatter(*point, f_func_problem_3(*point), color='r', s=50, label=f"{label} {point}")

# Show the plot
plt.legend()
plt.show()

